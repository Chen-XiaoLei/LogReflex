{
    "OutputCommitter set in config <*>": {
        "OutputCommitter is <*>": "No",
        "Putting shuffle token in serviceData": "No"
    },
    "Created MRAppMaster for application <*>": {
        "Registering class <*> for class <*>": "No",
        "MRAppMaster metrics system started": "No",
        "MRAppMaster launching normal, non-uberized, multi-container job job_<*>.": "No",
        "Starting Socket Reader #<*> for port <*>": "No",
        "Event Writer setup for JobId: job_<*>, File: hdfs://<*>": "No",
        "getResources() for application_<*>: ask=<*> release= <*> newContainers=<*> finishedContainers=<*> resourcelimit=<*> knownNMs=<*>": "No.",
        "Auth successful for <*> (auth:SIMPLE)": "No",
        "DFSOutputStream ResponseProcessor exception for block <*>:blk_<*>": "No"
    },
    "OutputCommitter is <*>": {
        "Emitting job history data to the timeline server is not enabled": "No",
        "Http request log for http.requests.mapreduce is not defined": "No",
        "maxTaskFailuresPerNode is <*>": "No",
        "blacklistDisablePercent is <*>": "No",
        "Size of <*> is <*>": "No"
    },
    "Emitting job history data to the timeline server is not enabled": {
        "Adding job token for job_<*> to jobTokenSecretManager": "No",
        "Not uberizing job_<*> because: not enabled; too many maps; too much input;": "No.",
        "Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server": "No",
        "Upper limit on the thread pool size is <*>": "No",
        "Retrying connect to server: <*>. Already tried <*> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<*>, sleepTime=<*> MILLISECONDS)": "No"
    },
    "Adding job token for job_<*> to jobTokenSecretManager": {
        "Input size for job <*> = <*>. Number of splits = <*>": "No",
        "Adding <*> tokens and <*> secret keys for NM use for launching container": "No",
        "Failed to renew lease for [<*>] for <*> seconds. Will retry shortly ...": "No"
    },
    "Input size for job <*> = <*>. Number of splits = <*>": {
        "Number of reduces for job <*> = <*>": "No"
    },
    "loaded properties from <*>": {
        "job_<*>Job Transitioned from NEW to INITED": "No",
        "Done acknowledgement from <*>": "No",
        "Diagnostics report from <*>: Container killed by the ApplicationMaster.": "No."
    },
    "Using mapred newApiCommitter.": {
        "Using callQueue class <*>": "No"
    },
    "Scheduled snapshot period at <*> second(s).": {
        "Instantiated MRClientService at <*>": "No"
    },
    "IPC Server Responder: starting": {
        "IPC Server listener on <*>: starting": "No"
    },
    "Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server": {
        "Logging to org.slf4j.impl.Log4jLoggerAdapter(<*>) via org.mortbay.log.Slf4jLog": "No",
        "Processing the event EventType: <*>": "No"
    },
    "Using callQueue class <*>": {
        "Added global filter 'safety' (class=<*>)": "No"
    },
    "Added global filter 'safety' (class=<*>)": {
        "Added filter <*> (class=<*>) to context mapreduce": "No"
    },
    "Added filter <*> (class=<*>) to context mapreduce": {
        "Added filter <*> (class=<*>) to context static": "No",
        "Added <*> to list of failed maps": "No"
    },
    "Logging to org.slf4j.impl.Log4jLoggerAdapter(<*>) via org.mortbay.log.Slf4jLog": {
        "Jetty bound to port <*>": "No",
        "Extract jar:<*>!/webapps/mapreduce to <*>": "No",
        "Connecting to ResourceManager at <*>": "No",
        "Resolved MSRA-SA-<*>.fareast.corp.microsoft.com to <*>": "No"
    },
    "Instantiated MRClientService at <*>": {
        "Web app /mapreduce started at <*>": "No"
    },
    "job_<*>Job Transitioned from NEW to INITED": {
        "job_<*>Job Transitioned from INITED to SETUP": "No",
        "task_<*> Task Transitioned from NEW to SCHEDULED": "No",
        "<*> TaskAttempt Transitioned from NEW to UNASSIGNED": "No"
    },
    "job_<*>Job Transitioned from INITED to SETUP": {
        "job_<*>Job Transitioned from SETUP to RUNNING": "No"
    },
    "maxContainerCapability: <memory:<*>, vCores:<*>>": {
        "mapResourceRequest:<memory:<*>, vCores:<*>>": "No",
        "reduceResourceRequest:<memory:<*>, vCores:<*>>": "No",
        "Recalculating schedule, headroom=<memory:<*>, vCores:<*>": "No",
        "Cannot assign container Container: [ContainerId: <*>, NodeId: <*>, NodeHttpAddress: <*>, Resource: <memory:<*>, vCores:<*>>, Priority: <*>, Token: Token { kind: ContainerToken, service: <*> }, ] for a map as either container memory less than required <memory:<*>, vCores:<*>> or no pending map tasks - maps.isEmpty=true": "No"
    },
    "Http request log for http.requests.mapreduce is not defined": {
        "Reduce slow start threshold not met. completedMapsForReduceSlowstart <*>": "No"
    },
    "Resolved <*> to <*>": {
        "Assigned container container_<*> to attempt_<*>": "No"
    },
    "Before Scheduling: PendingReds:<*> ScheduledMaps:<*> ScheduledReds:<*> AssignedMaps:<*> AssignedReds:<*> CompletedMaps:<*> CompletedReds:<*> ContAlloc:<*> ContRel:<*> HostLocal:<*> RackLocal:<*>": {
        "After Scheduling: PendingReds:<*> ScheduledMaps:<*> ScheduledReds:<*> AssignedMaps:<*> AssignedReds:<*> CompletedMaps:<*> CompletedReds:<*> ContAlloc:<*> ContRel:<*> HostLocal:<*> RackLocal:<*>": "No"
    },
    "Upper limit on the thread pool size is <*>": {
        "The job-jar file on the remote FS is hdfs://<*>": "No"
    },
    "The job-jar file on the remote FS is hdfs://<*>": {
        "The job-conf file on the remote FS is <*>": "No"
    },
    "<*> TaskAttempt Transitioned from NEW to UNASSIGNED": {
        "<*> TaskAttempt Transitioned from UNASSIGNED to ASSIGNED": "No"
    },
    "Processing the event EventType: <*>": {
        "Processing the event EventType: <*> for container <*> taskAttempt <*>": "No"
    },
    "Starting Socket Reader #<*> for port <*>": {
        "Shuffle port returned by ContainerManager for <*> : <*>": "No"
    },
    "IPC Server listener on <*>: starting": {
        "TaskAttempt: [<*>] using containerId: [<*> on NM: [<*>]": "No",
        "<*> failures on node <*>.com": "No"
    },
    "<*> TaskAttempt Transitioned from UNASSIGNED to ASSIGNED": {
        "<*> TaskAttempt Transitioned from ASSIGNED to RUNNING": "No"
    },
    "task_<*> Task Transitioned from NEW to SCHEDULED": {
        "task_<*> Task Transitioned from SCHEDULED to RUNNING": "No",
        "Task: attempt_<*> - exited : java.net.NoRouteToHostException: No Route to Host from <*> to <*> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost": "No"
    },
    "Executing with tokens:": {
        "JVM with ID : <*> asked for a task": "No",
        "Task succeeded with attempt <*>": "No"
    },
    "JVM with ID : <*> asked for a task": {
        "JVM with ID: <*> given task: <*>": "No",
        "Scheduling a redundant attempt for task task_<*>": "No"
    },
    "Size of <*> is <*>": {
        "Progress of TaskAttempt <*> is : <*>": "No"
    },
    "Assigned container container_<*> to attempt_<*>": {
        "Received completed container container_<*>": "No"
    },
    "Processing the event EventType: <*> for container <*> taskAttempt <*>": {
        "Container complete event for unknown container id container_<*>": "No"
    },
    "<*> TaskAttempt Transitioned from ASSIGNED to RUNNING": {
        "<*> TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP": "No"
    },
    "<*> TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP": {
        "<*> TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED": "No",
        "<*> TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP": "No"
    },
    "task_<*> Task Transitioned from SCHEDULED to RUNNING": {
        "<*> Task Transitioned from RUNNING to SUCCEEDED": "No"
    },
    "Received completed container container_<*>": {
        "Num completed Tasks: <*>": "No"
    },
    "Reduce slow start threshold not met. completedMapsForReduceSlowstart <*>": {
        "Reduce slow start threshold reached. Scheduling reduces.": "No"
    },
    "Number of reduces for job <*> = <*>": {
        "All maps assigned. Ramping up all remaining reduces:<*>": "No"
    },
    "Reduce slow start threshold reached. Scheduling reduces.": {
        "Slow ReadProcessor read fields took <*> (threshold=<*>); ack: seqno: <*> status: SUCCESS status: ERROR downstreamAckTimeNanos: <*>, targets: [<*>, <*>]": "No"
    },
    "DFSOutputStream ResponseProcessor exception for block <*>:blk_<*>": {
        "Error Recovery for block BP-<*>:blk_<*> in pipeline <*>, <*>: bad datanode <*>": "No"
    },
    "Slow ReadProcessor read fields took <*> (threshold=<*>); ack: seqno: <*> status: SUCCESS status: ERROR downstreamAckTimeNanos: <*>, targets: [<*>, <*>]": {
        "ERROR IN CONTACTING RM.": "No"
    },
    "Task: attempt_<*> - exited : java.net.NoRouteToHostException: No Route to Host from <*> to <*> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost": {
        "Diagnostics report from attempt_<*>: Error: java.net.NoRouteToHostException: No Route to Host from <*> to <*> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: <*>": "Yes"
    },
    "Task succeeded with attempt <*>": {
        "Task cleanup failed for attempt <*>": "No"
    },
    "Event Writer setup for JobId: job_<*>, File: hdfs://<*>": {
        "Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.<*>": "No"
    }
}